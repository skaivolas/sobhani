---
title: Grammar Construction in the Minimalist Program — Chapter 1 — Minimalism
layout: page
---

[Back](jherring-0) — [Forth](jherring-2)

# <span id="anchor-1"></span>Chapter 1 — Minimalism

## <span id="anchor-2"></span><span id="anchor-3"></span><span id="anchor-4"></span>Minimalist Assumptions

Minimalism is a program and not a theory. It makes no claims about which
formalisms are best suited to represent the structure of linguistic
knowledge. Strictly speaking, it doesn’t even make specific claims about
what constitutes linguistic knowledge. “There are minimalist questions,
but no minimalist answers, apart from those found in pursuing the
program: perhaps that it makes no sense, or that it makes sense but is
premature.” (Chomsky 2000) It is merely a set of guidelines for asking
questions, leavened with some speculation about what lines of inquiry
are likely to yield interesting results.

Taken at face value, this would seem to rule out any attempt to
formalize it. If it is not a theory, and if it does not make any
testable claims about how language works, it’s not clear what kinds of
objects any hypothetical formalization would operate over, let alone how
to evaluate any attempt to specify them. As the founder himself notes,
“It is a misunderstanding to contrast ‘minimalism and X,’ where X is
some theoretical conception (Optimality Theory, Lexicalism, etc.). X may
be pursued with minimalist goals, or not.” (Chomsky 2000) Minimalism
*per se *has no truth value.

And yet in practice, The Minimalist Program’s pedigree has ensured that
it strongly resembles a coherent theory. It finds its roots in the
*Government and Binding Theory *framework proposed in (Chomsky 1981) and
in subsequent work under the rubric of *Principles and Parameters
*(Haegeman 1994). This foundation is both conceptual and empirical (see
(Epstein and Hornstein 1999) for one confirming account).

Conceptually, the crucial shift involved dispensing with traditional,
rule-based grammar systems in favor of very general, universally
available and invariant operations which transform (syntactic)
structures that, in contradistinction to the more familiar
explicitly-stated, pattern- based structures of formal grammars with
construction-specific rules, are *projected from *lexical items (more
accurately, their categories) and combinations of lexical items.
Expressions generated by the transformational component are well-formed
or not according to how well they conform to a set of universal
constraints. (See (Chomsky 1995) for elaboration.)

On the empirical side, the tendency has been for Minimalist researchers
to build on work done in this tradition, taken to have been highly
successful at capturing important, non-obvious truths about human
language, and as such to be a good set of guideposts for uncovering the
principles of Universal Grammar. Minimalism treats *Principles and
Parameters *research generally as descriptively correct but ill-stated:
formulated in ways that distract from, or at least are not particularly
revealing about, *why *human language should be the way it is. (Chomsky
1995; Hornstein, Nunes, and Grohman 2005)

This question of *why *human language should be one way and not some
other animates minimalist inquiry. Minimalism is a program in the sense
that it fixes on no particular answer to this question, but rather seeks
to generate a family of theories that propose competing answers. In
practice, researchers have adopted the same one, however, formulated
succinctly as the **Strong Minimalist Thesis **of (Chomsky 2000):

Language is an optimal solution to legibility conditions

It may be more accurate to say that this has been a guiding principle of
the program than to call it a thesis. To the extent that it is a thesis,
one would need (a) some sense of how *language*, for the purpose of this
statement, is bounded in the cognitive domain, (b) some specifics about
how these *legibility conditions *are constituted pursuant to an
evaluation function ranging over solutions, and (c) a definition of
“optimal” for this purpose, since any hypothetical evalution function
will depend on it. None of these particulars are currently forthcoming
in anything but the vaguest of terms.

Neither were they promised. In an important sense, Minimalism “works
backward” compared with more familiar scientific approaches. It knows
what theory it would like to propose, but it is certain neither of the
specific formulation of this theory, nor even whether it is viable. The
pursuit is nevertheless thought worthwhile on the strong hunch that
*something like *the target theory will prove viable. If this turns out
to be true, the deviations from the original target may well be
illuminating in their own way. Human language is the way it is either
because it is an optimal solution to legibility conditions, or because
it would be if it could, but for something preventing it from being so,
and as we have a scientific curiosity about what that “something” is, it
will have been fortuitous that we phrased the question in just this way.

The “hunch” itself is based in a series of empirically grounded, if not
conclusively established, assumptions about the origins and nature of
natural language (Chomsky 2005). First, that it is a biological system
in the same sense that the visual or circulatory systems are, consisting
perhaps of some dedicated parts that are specific to language, but
mostly using and interfacing with organs that additionally serve other
purposes. Second, that it emerged late in human evolution, and quite
rapidly compared with other biological morphology. Third, that it is
reasonably modular — that is, that a “faculty of language” exists fairly
independently of other systems of the body.

If all this is true, it is reasonably clear in the abstract both what
“interface conditions” are and why they would be the arbiters of
linguistic output. Since the newly-evolved language system came late to
the game, comparatively little of its structure is specific to its
operation. It makes use of existing systems — such as arms and digits
for sign language, or the tongue, mouth and larynx for spoken language,
as well as the propositional apparatus of the brain for logical
interpretation, etc. Linguistic output must come in a form that these
systems can convert into useable (mental) objects, and with minimal
disruption of their preexisting mode of operation; the onus is squarely
on the language faculty to conform to the external systems. It is also
reasonably clear why we might expect the language faculty’s “solution”
to the problem of interfacing with the systems it has coopted for
external expression to be something like optimal. If it is true that
language’s appearance was rapid, it stands to reason that only a
*minimal *number of changes to preexisting primate biology are involved.
Explaining the outsized effectiveness of language is much easier if this
minimal number of changes were particularly well-adapted to the niche
they evolved to fill — i.e. that they “got it (mostly) right the first
time.”

Imagine some primate with the human mental architecture and sensorimotor
apparatus in place, but no language organ. It has our modes of
perceptual organization, our propositional attitudes . . . insofar as
these are not mediated by language, perhaps a “language of thought” in
Jerry Fodor’s sense, but no way to express its thoughts by means of
linguistic expressions Suppose some event reorganizes the brain in such
a way as, in effect, to insert FL. To be useable, the new organ has to
meet certain “legibility conditions.” (Chomsky 2000)

This “evolutionary fable” has consequences for how concepts like
“minimal” and “optimal” are to be understood in the program. It is
tempting, and a common mistake, to understand these exclusively in their
traditional guises as guidelines to reduce, as far as possible,
theoretical and operational complexity. Indeed, both of these senses are
operative in the Minimalist Program (and inherited from the *Principles
and Parameters *approach — see (Chomsky 2008) and (Gallego 2010)):
theorists should not multiply entities (in the sense of *lex
parsimoniae*

\- “Occam’s Razor”), and simpler operations that impose a lower
computational burden (involving fewer or less expensive “steps”) are to
be prefered to ones that are more involved. But there is the additional
*desideratum *that the operations and objects composing any minimalist
theory be independently biologically justified, a point which creates
some tension between the two traditional understandings of scientific
parsimony. If a proposed linguistic operation is complex or burdensome
compared with another, but is also better-grounded in
independently-motivated biological processes, the traditional preference
for the simpler operation is not decisive. Minimalism, therefore, is not
wedded to computational efficiency in the sense a computer scientist
might use the term. It is first and foremost a program of cognitive
biology, however contentious it may be for a group of researchers which
by and large lacks biological and psychological training to assume that
mantle. Consequently, computational linguistic researchers should not
necessarily expect minimalist theories to yield machine-efficient
results. The claim is rather that their results will be cognitively
accurate compared with other approaches to machine-based language
implementation (see (Hornstein and Idsardi 2014) for elaboration).

## <span id="anchor-5"></span><span id="anchor-6"></span><span id="anchor-7"></span>Minimalist Machinery

At least as regards the backbone architecture of the human language
faculty, agreement among minimalist researchers is near-universal. There
is assumed to be a set of linguistic features **F **which is universally
available and language-independent. These are features in the
“Bloomfieldian” sense of marking parameters along which linguistic
objects form oppositions with one another — the idea being that the
collection of features marks “what is special” about a particular item.
Individual languages **L **choose from among these features, so that
human languages will select a subset of those available in the universal
inventory, but never the entire set. There are no theories at present
about how this selection takes place, nor what the limits are, but there
is plenty of empirical work capturing apparent dependency
patterns/implicational hierarchies found in the resulting selection.
Features are bundled into lexical items — LI — which are collected into
a Lexicon — **Lex** — available to the language faculty. The language
faculty — FL — comprises this lexicon and a computational engine —
sometimes called *narrow syntax — *which applies operations from a set
of universally-available and invariant computational processes —
*C*<sub>HL</sub> — to generate a set of *expressions — ***\{Exp\}**.
Characterization of a language **L **as defining membership in a set of
expressions is common in formal language theory (Hopcroft, Motwani, and
Ullman 2001), and has been integral to Chomskyan approaches to natural
language since at least (Chomsky 1957), and arguably since (Harris
1951). It is indeed characteristic of most competing approaches as well.

Where Minimalism begins to distinguish itself from current approaches
and its immediate predecessors is in the way it characterizes *C*<sub>HL
</sub>and evaluates membership in **\{Exp\}**.

To the extent language is “an optimal solution to interface conditions,”
**\{Exp\} **should contain primarily — preferably *only — *material which
makes sense to the interfaces. Put differently, FL should output
information which is useful to other cognitive systems, and ideally
*nothing more*. Since, by assumption, input to narrow syntax is composed
of bundles of features from **F **(i.e. of *lexical items*), Minimalist
**\{Exp\} **consists only of arrangements or sequences of members of **F
**that other cognitive systems can use. A sequence of features that is
acceptable to a given set of external cognitive systems is said to
*converge*, and all others *crash *(Chomsky 1995) — terms which can be
taken as jargon for marking membership, and lack of same, in the
(infinite) set of expressions that characterizes any particular given
language.

Unfortunately for the progress of Language Science, it is not yet
possible to open up a human mind and, with any precision, take inventory
of its cognitive subsystems or say in any detail how they interact with
each other. “The Interfaces” must necessarily remain, for the
foreseeable future, theoretical constructs. Minimalism naturally takes
the conservative approach and tries to assume as little as possible
about them, postulating only two: the **A**rticulatory-**P**honetic (A-
P) interface — sometimes called **PF**, and the
**C**onceptual-**I**ntensional (C-I) interface — sometimes called **LF
**(Chomsky 1993). These constructs are conscious oversimplifications,
stand-ins for what are possibly arrays of cognitive subsystems of
varying degrees of applicability to language proper. They are
conceptualized in this way simply because of general agreement, spanning
the entire history of the study of language, that at a minimum grammars
pair “sound” (i.e. expression/articulation) and meaning (Chomsky 2005).

In principle, little more is required. The faculty of language could
simply consist of a set of linguistic features and a random sequencer
that *selects *all possible arrays of all possible lengths (the *power
set*) from its members, throws them at the interfaces and sees which
ones stick. But such a “solution” is unsatisfying for a number of
reasons. In addition to generating a bunch of linguistic “junk,” it
offers no insights into *why *some sequences of features work and others
don’t. It is in this space — the space between the existing set of
features and the interfaces that make use of them — that Minimalism
hopes to be illuminating.

Naturally, therefore, the lion’s share of the action of Minimalist
theorizing plays out in placing constraints on what operations comprise
*C*<sub>HL — </sub>i.e. constraints on what narrow syntax can do. It is
important to understand that “placing constraints” is not perfectly
reducible to “eliminating machinery.” If it were, the “random
generation” theory would be optimal. Minimizing machinery is
important, but it takes a back seat to minimizing the load on cognition
in general. Generating endless amounts of junk just to extract the
useful subparts is therefore out of bounds.

At least two aspects of the “random generator” baseline will necessarily
be retained: **Select **and the idea of a **Lexical Array**. The need
for some procedure that selects subsets of **Lex **is clear: the
individual natural language utterances we observe do not make use of the
full panoply of lexical items or features. Therefore, there is a mental
process that sections them off — call it **Select**. The existence of
**Lexical Array**s, subsets of lexical items selected as input to
individual firings of *C*<sub>HL</sub>, follows analytically (any
selection from a set is automatically a subset, which minimalists —
somewhat at odds with standard practice in formal language theory — call
an “array”).

Leaving the “random generator” approach behind requires something more,
however. With just **Select **into **Lexical Array**s that are input to
the operations of *C*<sub>HL</sub>, any operations of *C*<sub>HL
</sub>would be suboptimal departures from simply letting the ultimate
arbiters of convergence — the interfaces — have at the **Select**ed
arrays directly. Either **Select **must be more *selective*, or **Lex
**must have some structure over and above just being a repository for
features bundled into lexical items, or else narrow syntax does more
work than simply arranging features.

And in fact, Minimalism makes use of all three possibilities — the first
explicitly in the form of *lexical subarrays *which form the basis for
derivation in *phases *(see (Chomsky 2001), (Chomsky 2008)), and the
second implicitly with the assumption that certain lexical items, called
*core functional categories *(CFCs), are specially marked in the Lexicon
to form the nucleus for such subarrays, determining in part which
(classes of) other items can be present (see (Chomsky 1995)). What items
constitute the set of CFCs is a matter of some dispute, but the idea
that at least *C *(= the clause), *T *(= tense) and (various flavors of)
*v\* *(= the so-called “light verb”) are in the set is generally
accepted.<sup>\[1\]</sup>

The third — that narrow syntax adds interface-necessary information not
directly present in the lexicon — is universally accepted but a bit
contentious to state, as it flirts with violating an important proposed
economy condition, the **Inclusiveness Condition**:

Any structure formed by the computation is constituted of elements
already present in the lexical items selected for N\[umeration\]; no new
objects are added in the course of computation, apart from
rearrangements of lexical properties (in particular, no indices, bar
levels in the sense of X-bar theory, etc.) (Chomsky 1995)

In other words, narrow syntax isn’t allowed to add any *lexical
*information that wasn’t already there. In some important sense, all it
does is select and arrange feature bundles from the lexicon into
configurations that the interfaces can use. But there is at least one
important reason to believe it is a bit more than that: the ubiquitous
*displacement property *of natural language.

Possibly the only fundamental way in which natural languages differ in
form from artificial languages, such as programming languages, and a
factor present in all known human languages, the *displacement property
*is a fact that any theory of natural language must account for
(Hornstein, Nunes, and Grohman 2005). Put simply — unlike in artificial
languages, where sequential position uniquely determines interpretation,
in natural languages interpretation can be disjoint from sequential
position. In fact, elements can be interpreted in *multiple *positions
(though in the general case they are articulated in only one). The
target minimalist conclusion would obviously be that something about the
tension between the two interfaces — some difference in intelligibility
requirements between A-P and C-I — necessitates this. And in the ideal
case, not only is displacement born out of the tension between these two
systems, it further turns out to be the computationally optimal out of
all available ways of resolving such tension.

At present, there is of course no clear sense of why an “optimal”
solution to differing legibility requirements would require
displacement, but that is to be expected. Again, it is a target
conclusion, one the Minimalist Program would like to reach, not one for
which there is currently any compelling evidence. It takes the form, for
now, of a working hypothesis, one that guides the inventory of proposed
operations in *C*<sub>HL </sub>in important ways.

An obvious place to look for such a solution is in the fact that
conceptual relations seem naturally hierarchical while articulatory
relations are sequential. Perhaps, then, something about the need to
express hierarchical relations in sequential form tailors *C*<sub>HL
</sub>in a way that makes displacement a computational “good fit”.

Indeed, the core operations assumed by the vast majority of minimalist
theories fit that bill. These are: **Merge**, **Probe**, **Agree**,
**Value**, **Move**, and, with some mild controversy, also **Copy **and
**Delete**.

Of these, **Merge — **the operation that combines two<sup>\[2\]
</sup>syntactic objects into a composite syntactic object — is
fundamental. No theory of syntax which is even mildly lexicalist can
exist without a version of it. Typical minimalist theories take **Merge
**to be technically unconstrained (see (Gallego 2010)): **Merge **simply
takes two inputs and returns a single output. Consequently, it is not by
itself the locus of whatever constraints keep the system from
overgenerating (see (Boeckx 2008b)).<sup>\[3\] </sup>What it does do is
establish a number of relations that are variously believed to have
importance at the interfaces. The simplest of these is *sisterhood*,
which obtains between any two objects that have been **merge**d. Another
is *c-command*, which is established between one object and all the
subparts of an object with which it has **Merge**d. Since **Merge **is
containment-based — that is, it constructs syntactic objects out of
syntactic objects — the hierarchy that seems important for legibility at
the C-I interface comes as a by-product.

Constraints on **Merge **are realized — directly or otherwise — by
**Agree**, paired with either **Value **or **Delete **(or, perhaps
suboptimally, both). Since the program already assume the existence of
features, it is natural to suppose that features are what determine the
subset of arrangements of lexical items from **Lex **that are in **\{Exp\}
**By further assumption

(the **Inclusiveness Condition**), the operations of *C*<sub>HL
</sub>are forbidden from creating lexical information. Therefore,
featural constraints will either take the form of **Value**-ing features
— that is, determining which subclass a set of features representing the
parent class actually instantiates in the particular expression — or
**Delete**-ing them.

Either choice represents a mild violation of the spirit of the
**Inclusiveness Condition**. In the former case, this is because
determining that what was an abstract class is a concrete subclass at
least *reveals *information that wasn’t strictly present in the lexicon.
In the latter case, this is because it seems to violate the optimality
assumption. There is something *prima facie *suspect about the idea that
a lexicon optimized for communicating with external cognitive interfaces
encodes information those interfaces cannot use. Using at least one of
**Value **or **Delete **seems unavoidable, however, for, as noted above,
if narrow syntax were simply in the business of arranging features, the
random generator approach would be enough, and the theory not very
explanatory.

In truth, the violations in both cases are mild to the point of
irrelevance. Determining that a featural superclass has such-and-such a
value in a given expression is an operation that itself has an economy
interpretation: why should the lexicon create multiple lexical items
when a single one can be leveraged for a class of related purposes?
Likewise, the idea that the lexicon would encode information that one or
the other of the interfaces can’t use is, from a certain point of view,
not different from saying that operations of *C*<sub>HL </sub>are
necessary to appease the interfaces — an assumption that underlies any
justification for the existence of *C*<sub>HL </sub>*a priori*.

If **Agree **is to explain displacement, it is natural to assume that it
operates at a remove i.e. is not strictly local — and determining its
boundaries is arguably the core driver of minimalist research. So,
subject to the conditions imposed by an individual minimalist theory,
**Agree **is an operation that obtains between two lexical items — or
perhaps features specified on them — which are in some relationship with
each other resulting from a previous **Merge **operation. This could be
simple *sisterhood *or some more “distant,” perhaps *c-command*ing,
configuration.

Most accounts take **Agree **to be asymmetric: one half of the pair is a
*probe *and the other is a *goal*. The distinction is motivated in equal
parts by conceptual necessity and economy considerations. On the
conceptual side, regardless of whether one chooses the **Value **or
**Delete **route, the effects of the operation affect each party
differently, implying an asymmetry. On the economy side, the asymmetry
is useful in limiting the scope and number of operations: **Agree
**operations can be triggered only upon **Merge **of a *probe *(an item
with requirements to satisfy), for example, or perhaps are limited to
searching for goals among their lexical subarray.

As mentioned, it’s not too much of a stretch to say that what bounds are
placed on **Agree **relations are the real meat of any minimalist
theory. These are not exclusively configurational. Cyclic considerations
are also central, and it is precisely this function that *phases *serve.

*Phases *were introduced as a way of constraining the choice of
**Lexical Array**s, but this is only one of their purposes, and not
really the most important one. In the course of the derivation, they
additionally realize bounding conditions, delimiting the range over
which syntactic operations can apply, and also act as a means of
ordering operations, by requiring that operations involving members of
the subarray apply to them as a group before any member of the group is
allowed to interact with members outside. Implementational details of
*phases *can vary quite a bit among minimalist theories, but it is not
necessary to survey all of them (see (Citko 2014) for a fairly
comprehensive survey). The purposes are always the same: to constrain
the domain of application of a given operation in some particular way.

The final operation, and the one most directly implicated in explaining
*displacement*, is **Move**, sometimes called **PiedPipe**. This process
happens as a reflex of **Agree**, but not all instances of **Agree
**result in **Move**. When **Move **occurs, a syntactic category that
occupies one position in the assembled hierarchy is attached at a second
position determined by the *probe *member of the **Agree **relation that
triggered it. The word “attached’ is used advisedly, to emphasize that
the source instance of the **Move**d item is still represented in its
original position, just not overtly realized in any articulatory sense —
at least not in the general case (see (Boskovic and Nunes 2007) for
evidence that articulation of instances of objects that have
subsequently been re-attached by **Move **is sometimes grammatical).
Syntactic items can be *attached *at multiple points in the hierarchy
provided there is some motivation to do so, where”motivation" typically
means marking that an **Agree **relation has occurred that **Delete**d
or **Value**d features that would otherwise have caused a crash at one
or the other of the interfaces<sup>\[4\]</sup>.

Minimalist theories can, of course, add to this inventory as needed,
provided the proposed operations are empirically motivated and
conceptually deferential to minimalist concerns, but these are the basic
builing blocks. To review:

16) 1.  There is a *lexicon *which is a repository of linguistic
        features bundled into (atomic) lexical items. The lexicon is
        specific to a given language, but the features it bundles come
        from a universal inventory.
    2.  There is a set of operations *C*<sub>HL — </sub>assumed to be
        universally available and invariant across languages — which can
        be applied to items from the lexicon.
    3.  Operations of FL begin by **Select**ing items from this
        inventory into a **Lexical Array**.
    4.  **Select**ion is organized around special lexical items that
        comprise the class of *core functional categories*.
    5.  “Organized around” means that individual examples of CFCs guide
        **Select **by identifying numbers and categories of elements to
        choose from. Items so chosen end up in a *subarray *with the CFC
        instance.
    6.  *Subarray*s of the **Lexical Array **identify *phases*. *Phases
        *serve to limit the range over which *C*<sub>HL </sub>operations
        can apply and, in an indirect way, to constrain the order in
        which operations must apply in the formation of a longer
        utterance.
    7.  **Merge **is the fundamental structure-building operation,
        responsible for establishing relationships between items in the
        **Lexical Array**. It is in principle unconstrained, but some
        choices of pairs of items to **Merge **may end up blocking, as a
        side-effect, **Agree **operations between other items that need
        to happen for the derivation to converge.
    8.  A language consists of all the expressions constructable from a
        *lexicon *under these constraints that *converge *at all the
        interfaces.
    9.  An expression *converges *if it contains only information which
        is legible at the interfaces. In practice, this means all of its
        uninterpretable features have been eliminated, or unvalued
        features have been valued, at some point before the end of the
        dervivation.
    10. The primary method of restricting *convergence *among the model
        objects to those that correspond to grammatical utterances in a
        real language is by devising ways to block **Agree**. These
        blocks typically take the form of limiting search space — i.e.
        are implemented through conditions established by *phases*, or
        are established through intervention effects — configurations
        where an unsuitable *goal *blocks a *probe *from finding a
        better match.

## <span id="anchor-8"></span>The Purpose of this Project

Perhaps the most striking feature of all of this machinery to people
unfamiliar with it is how imaginative it seems. While it is true that
nothing presented here is without conceptual motivation, it is equally
true that there is no direct way of verifying the cognitive reality of
any of it. It is all speculation about how the faculty of language might
operate, and that only if certain guiding assumptions turn out to be
correct, which they may well not.

The ontological status of things like *phase*s and **Merge **is in an
important sense left up to the individual reader. They could be
blueprints — plans for how to build the eventual theory. They could be
scaffolding — temporary fixtures which enable construction of the real
theory. They could be placeholders — ways of talking about families of
mechanisms yet to be determined. Or they could even, in the fortunate
case, be direct analogues of cognitive processes that will turn out to
be only slightly messier than these idealized versions.

What they cannot be is directly observed.

That’s alright. The lack of direct access to the object of study is no
secret, and the hypothetical nature of the object of study is hardly a
unique condition in the sciences. Many things about the world that we
think we know have essentially the same status. What is available is a
vast amount of easily-obtainable lingusitic data. Minimalism’s
theoretical constructs are useful to the extent they account for that
data, and that is all.

What perhaps does set Linguistics a bit apart is the comparative lack of
a concrete toolset. Physicists work with constructs that are easily as
imaginatve — and certainly at starker odds with everyday experience — as
those syntacticians devise. If Physics has a better reputation for
objectivity, then because physicists can typically be more explicit
about what it means for a theory to fail, and that often because the
predicted effects are isolable.

In Syntax, comparatively little is isolable. The object of study itself
— FL — is mediated through layers of cognitive machinery of varying
degrees of linguistic specialization. On top of that, every utterance,
by hypothesis, is formed from the same narrow band of operations on
features. There is the very real possibility of unintended consequences,
therefore: minor changes in assumptions about these devices to account
for one observed phenomenon may well (and often do) silently disturb
assumptions made previously to account for other phenomena.

For that reason, internal consistency is the best yardstick for success
a minimalist researcher has. Theories are successful to the extent that
they account for large amounts of data with minimal amounts of side
effects (undesireable results requiring stipulative repair). The trouble
is that there is no easy way to test for “side effects” short of
mentally going through the motions of deriving fleets of expressions
with the proposed new device.

The purpose of this project is to add to the syntax researcher’s working
toolset a way to automate such evaluations, so that they can be applied
quickly to larger and more varied sets of data than an individual
researcher typically thinks to include in his own mental checks. This
brings benefits on a number of levels. For one, by building a standard
set of data sentences, researchers working in parallel can quickly
determine what the relative strengths and weaknesses of their individual
proposals are, especially beyond the sets of data with which they are
directly familiar. In the minimalist world, broader coverage isn’t the
only metric for the success of a theory, but even in cases where
researchers trade coverage for perceived cognitive accuracy, it can be
useful to know what areas are not covered as a jumping off point for
further investigation. More importantly, though, the exercise of
specifying a theory in enough detail that a machine can “run” it over
data is an invaluable clarifying device. Programming environments are
less forgiving than a human observer, as programming environments are
*tabula rasa*. They don’t — can’t — share intuitions or assumptions.
Forcing oneself to lay bare intuitions and assumptions can by itself
lead to insights. Pointedly, it is exactly this kind of exercise,
applied to *Government and Binding Theory *results, that was the bread
and butter of the early Minimalist Program.

Of course, the open-ended nature of the Minimalist Program — the fact
that it is a program and not a theory — makes full realization of this
project perhaps impossible. But like the program it augments, full
realization is maybe also not the point. If the exercise itself brings
insights and gets the program closer to being able to state the target
theory in the appropriate detail, it is a success.

## <span id="anchor-9"></span><span id="anchor-10"></span><span id="anchor-11"></span>Roadmap

The remainder of this presentation is divided into two sections.

The first of these, comprised of chapters 2 and 3, surveys core
mechanisms. As noted above, it is not possible to properly delineate the
domain of grammatical operations that are allowed for minimalist
analysis. For this reason, this project is in an important sense
impossible. What is possible is to justify the choice of core operations
that any conceivable “minimalist” toolkit should include, and that is
what these chapters aim to do.

Each are based in historical survey. Because Minimalism is not a formal
theory, it is, in a sense, its lineage. Not unlike the “undocumented
constitution” of the United Kingdom, it has a few foundational texts
augmented with years of precedent, practice and referenced authority,
rather than any definitive set of formal commitments. Understanding how
it came to be the way it is is critical in characterizing the field.

In its current iteration, Minimalist analysis is organized primarily
around Phases and Agree. This section therefore deals with each in turn.

Chapter 2 deals with Phases — where they came from, where they stand,
and what purposes they serve. It is ultimately concluded that *phase *in
the current understanding is a bit of a hodgepodge. The category bundles
together ideas that are conceptually separate: (operation) ordering,
grouping, and locality. The current system provides mechanisms for
separating these concerns.

Chapter 3 deals with Agree. Though Minimalism is, on the surface, a
fully lexicalist program, it retains traces of its heritage in the
Gen-Eval tradition of GB. In contradistinction to most formal theories
of grammar, combinations of items are in principle unrestricted in
Minimalism. What guards against “illicit” combinations are not so much
prohibitions on combinations themselves as the consequences that arise
from them. Deciding what those consequences are is the purview of Agree,
making Agree the primary determinate of “grammaticality” in Minimalism.
Chapter 3 motivates a value-based concept of Agree with a survey of
current emprical arguments and concludes by outlining what Agree
mechanisms have to be present to capture the insights of modern
Minimalist theories.

These motivational sections are followed by a concrete introduction to
the implemented toolkit. Chapter 4 outlines the programatic objects and
mechanisms involved, and which form the core of the tools available to
users. Chapter 5 then follows with a high-level example of the system’s
utility as applied to longstanding disputes around the Movement Theory
of Control.

More concrete examples of system operation can be found in a series of
appendices. Appendix A provides step-by-step recreation of one of the
analyses in Chapter 5 in sufficient detail that curious readers can
duplicate system functionality in a competing program. Appendix B
provides examples of ways derivations can fail and discussion of how to
repair them. Appendix C supplies the lexicons and starting lexical
arrays used in many of the examples presented, including the two main
ones.

This writeup then concludes with some discussion of insights into the
Minimalist Program gained over the course of the project and an outline
of how the project should be expanded in the future.

[Back](jherring-0) — [Forth](jherring-2)
